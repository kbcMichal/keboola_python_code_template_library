{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bebc843f-2616-4381-a853-73b76802a4c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Cleaning and Transformation\n",
    "\n",
    "This notebook provides practical examples of data cleaning and transformation tasks using Python libraries such as Pandas and NumPy. We'll use the Keboola library to load input data and produce outputs. \n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Load Input Data**: Use Keboola's CommonInterface to load data from input tables.\n",
    "2. **Data Cleaning**: Handle missing values, remove duplicates, and correct data types.\n",
    "3. **Data Transformation**: Normalize data, merge datasets, and create new features.\n",
    "4. **Save Output Data**: Write the cleaned and transformed data back to Keboola Storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120e61a-b591-4c63-8bbf-0da6394445b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keboola.component import CommonInterface\n",
    "import logging\n",
    "\n",
    "# Initialize CommonInterface\n",
    "ci = CommonInterface()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load input tables\n",
    "input_tables = ci.get_input_tables_definitions()\n",
    "\n",
    "# Check if input tables are available\n",
    "if not input_tables:\n",
    "    logging.error(\"No input tables found. Please configure Table Input Mapping in the workspace configuration.\")\n",
    "else:\n",
    "    # Load the first input table into a DataFrame\n",
    "    input_table_path = input_tables[0].full_path\n",
    "    df = pd.read_csv(input_table_path)\n",
    "    logging.info(f\"Data loaded from {input_table_path}\")\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecda07-5a8a-4ba4-800b-85c009abbcc8",
   "metadata": {},
   "source": [
    "### Load dataset from URL to follow the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58f798-4baa-4b6e-a24e-b7a34e6f572b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# URL of the Titanic dataset\n",
    "titanic_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "\n",
    "# Load the Titanic dataset into a pandas DataFrame\n",
    "df = pd.read_csv(titanic_url)\n",
    "\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac68920-1523-4e6f-8de3-aa784a00e83b",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "In this section, we'll handle missing values, remove duplicates, and correct data types to ensure the data is clean and ready for analysis.\n",
    "\n",
    "1. **Handling Missing Values**: We'll fill missing values in numeric columns with the mean of each column. For non-numeric columns, we'll fill missing values with a placeholder or drop them if necessary.\n",
    "2. **Removing Duplicates**: We'll remove any duplicate rows from the DataFrame.\n",
    "3. **Correcting Data Types**: We'll ensure that columns have the correct data types, such as converting columns to datetime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb7810-c6de-428d-b49f-0889bc67015e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "\n",
    "# Fill missing values for numeric columns with the mean of the column\n",
    "df.fillna(df.select_dtypes(include=[np.number]).mean(), inplace=True)\n",
    "\n",
    "# Fill missing values for non-numeric columns with a placeholder or drop them\n",
    "# Example: Replace NaN with 'unknown' for non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "df[non_numeric_cols] = df[non_numeric_cols].fillna('unknown')\n",
    "\n",
    "# Removing duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Correcting data types\n",
    "# Replace 'date_column' with the actual column name in your dataset\n",
    "if 'date_column' in df.columns:\n",
    "    df['date_column'] = pd.to_datetime(df['date_column'])\n",
    "\n",
    "logging.info(\"Data cleaning completed.\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0068d9-a85d-4711-aad1-9c0e15afd33b",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "Next, we'll perform data normalization, merge datasets, and create new features to transform the data for better analysis.\n",
    "\n",
    "1. **Data Normalization**: We'll normalize numeric columns to have a mean of 0 and a standard deviation of 1.\n",
    "2. **Merging Datasets**: If there are multiple tables, we'll merge them on a common key.\n",
    "3. **Creating New Features**: We'll create new columns based on existing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc3e87-223a-416c-b630-8708ab2c2fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "# Normalize numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = (df[numeric_cols] - df[numeric_cols].mean()) / df[numeric_cols].std()\n",
    "\n",
    "# Merging datasets\n",
    "# If there's another table to merge, specify the path and the common key for merging\n",
    "if len(input_tables) > 1:\n",
    "    second_table_path = input_tables[1].full_path\n",
    "    df2 = pd.read_csv(second_table_path)\n",
    "    \n",
    "    # Replace 'common_key' with the actual column name used for merging\n",
    "    df_merged = pd.merge(df, df2, on='common_key')\n",
    "    logging.info(f\"Data merged from {second_table_path}\")\n",
    "    display(df_merged.head())\n",
    "\n",
    "# Creating new features\n",
    "# Replace 'existing_column1' and 'existing_column2' with the actual column names in your dataset\n",
    "if 'existing_column1' in df.columns and 'existing_column2' in df.columns:\n",
    "    df['new_feature'] = df['existing_column1'] + df['existing_column2']\n",
    "\n",
    "logging.info(\"Data transformation completed.\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4109e-3c24-4126-99f4-a7cbd0c5d891",
   "metadata": {},
   "source": [
    "### Save Output Data\n",
    "\n",
    "Finally, we'll write the cleaned and transformed data back to Keboola Storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca7069-1ec4-47d2-b8c6-08ca4e04089c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define output table path\n",
    "output_table_name = \"cleaned_transformed_data.csv\"\n",
    "output_table_def = ci.create_out_table_definition(output_table_name, primary_key=['id'], incremental=False, destination=f'out.c-output.{output_table_name}')\n",
    "output_table_path = output_table_def.full_path\n",
    "\n",
    "# Save the processed DataFrame to the output path\n",
    "df.to_csv(output_table_path, index=False)\n",
    "logging.info(f\"Processed data saved to {output_table_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
