{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Profiling Notebook\n",
    "\n",
    "## Introduction\n",
    "This notebook is designed to help users perform data exploration and profiling with ease. By following the steps provided, you can load a dataset, generate detailed reports, and visualize your data. This process will help you understand the structure, relationships, and key characteristics of your data, facilitating better decision-making for further analysis or modeling.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Install Required Package\n",
    "In this step, we will install the `ydata-profiling` package, which is essential for generating data profiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --user ydata-profiling\n",
    "!pip install --user pyod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "### Import Libraries\n",
    "Here, we import all the necessary libraries for data analysis and visualization. This includes pandas for data manipulation, matplotlib and seaborn for plotting, and other utility libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib\n",
    "import sys\n",
    "import site\n",
    "import logging\n",
    "\n",
    "%matplotlib inline\n",
    "sys.path.append(site.getusersitepackages())\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from ipywidgets import widgets, Layout\n",
    "from keboola.component import CommonInterface\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "### Selecting a Dataset\n",
    "\n",
    "In this step, we will list the tables that users have loaded into the workspace using the table input mapping. Users can then select the dataset they want to use for data profiling and exploration.\n",
    "\n",
    "The input datasets are loaded using the Keboola Common Interface, which allows seamless interaction with the data tables defined in the workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize CommonInterface\n",
    "ci = CommonInterface()\n",
    "\n",
    "# Load input tables\n",
    "input_tables = ci.get_input_tables_definitions()\n",
    "\n",
    "# List all CSV files in the input tables directory\n",
    "table_list = []\n",
    "for table in input_tables:\n",
    "    table_list.append(table.full_path)\n",
    "\n",
    "# Create a dropdown widget for selecting a table\n",
    "if table_list:\n",
    "    logging.info(\"Select the dataset you want to use from the dropdown.\")\n",
    "    tables = widgets.Dropdown(options=table_list, value=table_list[0],\n",
    "                              description='Table:', disabled=False)\n",
    "    display(tables)\n",
    "else:\n",
    "    logging.warning(\"No tables found. Please ensure you have loaded tables into the workspace using the table input mapping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Selected Dataset\n",
    "Once you have selected a dataset from the dropdown, this cell reads the CSV file into a pandas DataFrame and generates a profile report using the `ydata-profiling` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(tables.value)\n",
    "profile = ProfileReport(data)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Alternatively Load Dataset from URL to follow the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Titanic dataset\n",
    "titanic_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "\n",
    "# Load the Titanic dataset into a pandas DataFrame\n",
    "data = pd.read_csv(titanic_url)\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Generate Profile Report\n",
    "This cell generates a comprehensive HTML report of the selected dataset. \n",
    "\n",
    "**Warning**: For large datasets, this process may consume a significant amount of memory and might fail. Ensure your dataset is of a manageable size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile.to_file(output_file ='/data/profile.html')\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Remove Redundant Columns\n",
    "\n",
    "### Identify Columns with Only Unique Values\n",
    "**NOTE:** Such columns are not useful for binary classification models.\n",
    "\n",
    "In this section, we will identify columns that contain only unique values. These columns are not helpful for binary classification as they do not provide any discriminative power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uniqueCols = []\n",
    "for col in data.columns:\n",
    "    if len(data[col].unique()) == len(data[col]):\n",
    "        uniqueCols.append(col)\n",
    "if len(uniqueCols) > 0:\n",
    "    print('[WARNING]', 'Columns', uniqueCols, 'contain only unique values, consider editting/grouping the data before using in binary classification model.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify columns with only unique values\n",
    "unique_cols = [col for col in data.columns if data[col].nunique() == len(data)]\n",
    "if unique_cols:\n",
    "    logging.info(f'[WARNING] Columns {unique_cols} contain only unique values. Consider editing/grouping the data before using in a binary classification model.')\n",
    "else:\n",
    "    logging.info('[INFO] No columns with only unique values found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns You Don't Want to Use\n",
    "**NOTE:** You can select multiple columns.\n",
    "\n",
    "Below is an interactive widget that allows you to select and drop columns that you do not wish to use in your analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dropdown widget for selecting columns to drop\n",
    "print(\"Select columns you want to drop:\")\n",
    "cols_to_drop = widgets.SelectMultiple(\n",
    "    options=data.columns,\n",
    "    rows=10,\n",
    "    description='Columns:',\n",
    "    disabled=False\n",
    ")\n",
    "display(cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Column Drop\n",
    "Execute the cell below to drop the selected columns from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop selected columns\n",
    "def drop_selected_columns(df, cols):\n",
    "    cols_to_drop_list = list(cols.get_interact_value())\n",
    "    if not cols_to_drop_list:\n",
    "        logging.info('[INFO] All columns will be kept in the data-frame for further exploration.')\n",
    "    else:\n",
    "        df.drop(columns=cols_to_drop_list, inplace=True)\n",
    "        for col in cols_to_drop_list:\n",
    "            logging.info(f'[INFO] Column {col} dropped.')\n",
    "\n",
    "# Apply the function with the selected columns\n",
    "drop_selected_columns(data, cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Solve Duplicates\n",
    "**Drop:** Will drop the duplicate rows.  \n",
    "**Keep:** Will keep the duplicate rows in the dataset.\n",
    "\n",
    "In this section, we will identify duplicate rows in the dataset and provide an option to either drop or keep them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows\n",
    "duplicates_cnt = len(data) - len(data.drop_duplicates())\n",
    "logging.info(f'[INFO] There are [{duplicates_cnt}] duplicates in the dataset...')\n",
    "\n",
    "# Create a toggle button for dropping or keeping duplicates\n",
    "drop_duplicates = 'Keep'\n",
    "if duplicates_cnt > 0:\n",
    "    drop_duplicates = widgets.ToggleButtons(\n",
    "        options=['Drop', 'Keep'],\n",
    "        description='Duplicates:',\n",
    "        disabled=False,\n",
    "        button_style='info',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltips=['Duplicate rows will be dropped', 'Duplicate rows will be kept'],\n",
    "        value='Keep'\n",
    "    )\n",
    "    display(drop_duplicates)\n",
    "else:\n",
    "    logging.info('[INFO] No duplicates found in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Duplicate Handling\n",
    "Execute the cell below to apply the chosen option for handling duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the chosen option for handling duplicates\n",
    "if drop_duplicates.value == 'Drop':\n",
    "    data = data.drop_duplicates()\n",
    "    logging.info('[INFO] Duplicate rows have been dropped.')\n",
    "else:\n",
    "    logging.info('[INFO] Duplicate rows have been kept.')\n",
    "\n",
    "# Re-check the number of duplicate rows\n",
    "duplicates_cnt = len(data) - len(data.drop_duplicates())\n",
    "logging.info(f'[DONE] There are [{duplicates_cnt}] duplicates remaining in the dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Encoding of Categorical Variables\n",
    "\n",
    "Label encoding has the advantage that it is straightforward but it has the disadvantage that the numeric values can be “misinterpreted” by the algorithms. For example, the value of 0 is obviously less than the value of 4 but does that really correspond to the data set in real life?\n",
    "\n",
    "In this section, we will:\n",
    "1. Identify different types of columns in the dataset (date, categorical, numeric).\n",
    "2. Handle missing values.\n",
    "3. Apply one-hot encoding to categorical columns.\n",
    "4. Apply label encoding to selected categorical columns.\n",
    "\n",
    "Let's start by identifying the column types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to identify data types\n",
    "def getDataTypes(data):\n",
    "    allColumns = list(data.columns)\n",
    "    datePreds = []\n",
    "    categoricalPreds = []\n",
    "    numericPreds = []\n",
    "    \n",
    "    for predictor in allColumns:\n",
    "        if data[predictor].dtype == 'object':\n",
    "            try:\n",
    "                pd.to_datetime(data[predictor])\n",
    "                datePreds.append(predictor)\n",
    "            except:\n",
    "                categoricalPreds.append(predictor) \n",
    "        elif 'datetime' in str(data[predictor].dtype):\n",
    "            datePreds.append(predictor)\n",
    "        else:\n",
    "            numericPreds.append(predictor)\n",
    "    \n",
    "    print('=====================================')\n",
    "    print('Dataset contains the following data types:')\n",
    "    print('Date variables:', '[', len(datePreds), ']', datePreds)\n",
    "    print('-------------------------------------')\n",
    "    print('Categorical variables:', '[', len(categoricalPreds), ']', categoricalPreds)\n",
    "    print('-------------------------------------')\n",
    "    print('Numeric variables:', '[', len(numericPreds), ']', numericPreds)\n",
    "    print('=====================================')\n",
    "\n",
    "    return datePreds, categoricalPreds, numericPreds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datePreds, categoricalPreds, numericPreds = getDataTypes(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "A common approach to encode categorical variables is called one-hot encoding. This method converts each category value into a new column and assigns a 1 or 0 (True/False) value to the column. This prevents misinterpretation of values by the algorithm but adds more columns to the dataset.\n",
    "\n",
    "Select the categorical columns you want to one-hot encode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget for selecting columns for one-hot encoding\n",
    "cols_to_ohe = widgets.SelectMultiple(\n",
    "    options=categoricalPreds,\n",
    "    rows=10,\n",
    "    description='Columns:',\n",
    "    disabled=False\n",
    ")\n",
    "display(cols_to_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply One-Hot Encoding\n",
    "Execute the cell below to apply one-hot encoding to the selected columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding\n",
    "data = pd.get_dummies(data, columns=list(cols_to_ohe.value))\n",
    "logging.info('[INFO] One-hot encoding applied to selected columns.')\n",
    "datePreds, categoricalPreds, numericPreds = getDataTypes(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Label encoding converts each value in a column to a number. While straightforward, it can cause the numeric values to be misinterpreted as having an inherent order.\n",
    "\n",
    "Select the categorical columns you want to label encode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget for selecting columns for label encoding\n",
    "cols_to_le = widgets.SelectMultiple(\n",
    "    options=categoricalPreds,\n",
    "    rows=10,\n",
    "    description='Columns:',\n",
    "    disabled=False\n",
    ")\n",
    "display(cols_to_le)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Label Encoding\n",
    "Execute the cell below to apply label encoding to the selected columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Apply label encoding\n",
    "for col in list(cols_to_le.value):\n",
    "    lb_make = LabelEncoder()\n",
    "    data[col + '_code'] = lb_make.fit_transform(data[col])\n",
    "    data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "logging.info('[INFO] Label encoding applied to selected columns.')\n",
    "\n",
    "datePreds, categoricalPreds, numericPreds = getDataTypes(data)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Solve Missing Values\n",
    "\n",
    "### Identify Missing Values\n",
    "\n",
    "In this section, we will identify the missing values in the dataset. This will help us understand the extent of missing data and decide on an appropriate action to handle it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to identify missing values\n",
    "def getMissing(data):\n",
    "    missing_cnt = data.isna().sum().sum()\n",
    "    missing_pct = missing_cnt / (len(data.columns) * len(data))     \n",
    "    missing_out = data.isna().sum()\n",
    "    \n",
    "    print('=====================================')\n",
    "    print(f'Total missing cells: [{missing_cnt}]')\n",
    "    print(f'Percentage of missing cells: [{missing_pct:.2%}]')\n",
    "    print('=====================================')\n",
    "    print('Count of missing cells per column:')\n",
    "    print(missing_out)\n",
    "    print('=====================================')\n",
    "    print('-------------------------------------')\n",
    "\n",
    "# Identify missing values in the dataset\n",
    "getMissing(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decide How to Handle Missing Values\n",
    "\n",
    "Choose a missing action from the following options:\n",
    "- **\"drop\"**: Drop rows with missing values in the selected column(s).\n",
    "- **\"replace\"**: Replace missing numeric values with the MEAN and missing categorical values with a new category named \"Undefined\" for the selected columns.\n",
    "- **\"replaceNumeric\"**: Replace missing numeric values with the MEAN value for the selected columns.\n",
    "- **\"replaceCategorical\"**: Replace missing categorical values with a new category named \"Undefined\" for the selected columns.\n",
    "- **\"None\"**: Ignore missing values.\n",
    "\n",
    "<h3><font color=\"red\">↓↓↓ Execute the cell below and choose how to solve missing values ↓↓↓</font></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display widgets to choose how to handle missing values\n",
    "if data.isna().sum().sum() > 0:\n",
    "    MISSING_ACTION = widgets.ToggleButtons(\n",
    "        options=['None', 'drop', 'replace', 'replaceNumeric', 'replaceCategorical'],\n",
    "        description='Action:',\n",
    "        disabled=False,\n",
    "        button_style='info',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "        value='None'\n",
    "    )\n",
    "    COLUMNS_ACTION = widgets.SelectMultiple(\n",
    "        options=['ALL COLUMNS'] + list(data.columns),\n",
    "        description='Columns:',\n",
    "        ensure_option=True,\n",
    "        disabled=False,\n",
    "        rows=15\n",
    "    )\n",
    "\n",
    "    display(MISSING_ACTION)\n",
    "    display(COLUMNS_ACTION)\n",
    "else:\n",
    "    logging.info('[INFO] There are no missing values in your dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Apply Missing Values Action\n",
    "\n",
    "Execute the cell below to apply the chosen action for handling missing values.\n",
    "\n",
    "<i><b>NOTE:</b> You can select and execute the missing action multiple times.</i><br>\n",
    "<i>For example, you can first select 'drop' for a specific column and then 'replaceNumeric' for numeric columns you prefer not to drop.</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Function to handle missing values based on selected action\n",
    "def solveMissing(data, MISSING_ACTION):\n",
    "    messageOut = []\n",
    "    allColumns = list(data.columns)\n",
    "    datePreds = []\n",
    "    categoricalPreds = []\n",
    "    numericPreds = []\n",
    "    \n",
    "    for predictor in allColumns:\n",
    "        if data[predictor].dtype == 'object':\n",
    "            try:\n",
    "                pd.to_datetime(data[predictor])\n",
    "                datePreds.append(predictor)\n",
    "            except:\n",
    "                categoricalPreds.append(predictor) \n",
    "        elif 'datetime' in str(data[predictor].dtype):\n",
    "            datePreds.append(predictor)\n",
    "        else:\n",
    "            numericPreds.append(predictor)\n",
    "    \n",
    "    if 'None' in MISSING_ACTION[:4]:\n",
    "        messageOut.append('Not solving any columns.')\n",
    "        \n",
    "    if MISSING_ACTION == \"replaceAll\":\n",
    "        messageOut.append('Replacing missing values in all columns:')\n",
    "        for col in allColumns:\n",
    "            if data[col].isna().sum() > 0:\n",
    "                if col in numericPreds:\n",
    "                    data[col].fillna(data[col].mean(), inplace=True)\n",
    "                else:\n",
    "                    data[col].fillna('REPLACED-Undefined', inplace=True)\n",
    "                messageOut.append(col)\n",
    "            \n",
    "    elif \"replaceNumeric\" in MISSING_ACTION:\n",
    "        messageOut.append('Replacing missing values in NUMERIC columns:')\n",
    "        for col in numericPreds:\n",
    "            if data[col].isna().sum() > 0:\n",
    "                data[col].fillna(data[col].mean(), inplace=True)            \n",
    "                messageOut.append(col)\n",
    "            \n",
    "    elif \"replaceCategorical\" in MISSING_ACTION:\n",
    "        messageOut.append('Replacing missing values in CATEGORICAL columns:')\n",
    "        for col in categoricalPreds:\n",
    "            if data[col].isna().sum() > 0:\n",
    "                data[col].fillna('REPLACED-Undefined', inplace=True)\n",
    "                messageOut.append(col)\n",
    "    \n",
    "    elif \"replace\" in MISSING_ACTION:\n",
    "        messageOut.append('Replacing missing values in selected columns.')\n",
    "        colsToReplace = ast.literal_eval(MISSING_ACTION.replace(\"replace\", \"\"))\n",
    "        for col in colsToReplace:\n",
    "            if col in categoricalPreds:\n",
    "                if data[col].isna().sum() > 0:\n",
    "                    data[col].fillna('REPLACED-Undefined', inplace=True)\n",
    "                    messageOut.append(col)\n",
    "            else:\n",
    "                if data[col].isna().sum() > 0:\n",
    "                    data[col].fillna(data[col].mean(), inplace=True)            \n",
    "                    messageOut.append(col)\n",
    "                        \n",
    "    if MISSING_ACTION == 'dropAll':\n",
    "        messageOut.append('Dropping missing values in all columns.')\n",
    "        data.dropna(inplace=True)\n",
    "            \n",
    "    elif \"drop\" in MISSING_ACTION[:4]:\n",
    "        messageOut.append('Dropping missing values in selected columns.')\n",
    "        colsToDrop = ast.literal_eval(MISSING_ACTION.replace(\"drop\", \"\"))\n",
    "        data.dropna(subset=colsToDrop, inplace=True)\n",
    "        messageOut.append(colsToDrop)\n",
    "    \n",
    "    if len(messageOut) == 0:\n",
    "        messageOut.append('[INFO] There is nothing to do for selected action.')\n",
    "    print(messageOut)\n",
    "    return data\n",
    "\n",
    "# Apply the chosen action for handling missing values\n",
    "missing_action_value = MISSING_ACTION.value\n",
    "columns_action_value = list(COLUMNS_ACTION.value)\n",
    "if 'ALL COLUMNS' in columns_action_value:\n",
    "    missing_action_concat = missing_action_value + 'All'\n",
    "else:\n",
    "    missing_action_concat = missing_action_value + str(columns_action_value)\n",
    "\n",
    "data = solveMissing(data, missing_action_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Check the basic details of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDescribe = data.describe(include='all').transpose()\n",
    "dataDescribe['ColumnName'] = dataDescribe.index\n",
    "\n",
    "display(dataDescribe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Anomaly Detection\n",
    "\n",
    "### Skewness and Kurtosis Analysis (of Numeric Variables)\n",
    "\n",
    "**Skewness** is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n",
    "\n",
    "**Kurtosis** is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. Data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers.\n",
    "\n",
    "In this section, we will:\n",
    "1. Calculate skewness and kurtosis for numeric variables.\n",
    "2. Identify columns that are highly skewed or peaked.\n",
    "\n",
    "Execute the cell below to perform the skewness and kurtosis analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate skewness and kurtosis\n",
    "def getSkewKurt(data):\n",
    "    dataSkewKurt = pd.DataFrame({\"ColumnName\": [], \"Skewness\": [], \"Kurtosis\": []})\n",
    "    for col in data.columns:\n",
    "        colSkew = data[col].skew()\n",
    "        colKurt = data[col].kurt()\n",
    "        dataSkewKurtTemp = pd.DataFrame({\"ColumnName\": [col], \"Skewness\": [colSkew], \"Kurtosis\": [colKurt]})\n",
    "        dataSkewKurt = dataSkewKurt.append(dataSkewKurtTemp, ignore_index=True)\n",
    "        if colSkew < -2 or colSkew > 2:\n",
    "            print(f'[WARNING] The {col} column is highly skewed. This can affect the performance of binary classification. Check the dataset if the share of outliers is acceptable.\\n')\n",
    "        if colKurt < -3 or colKurt > 3:\n",
    "            print(f'[WARNING] The {col} column is highly peaked. This can affect the performance of binary classification. Check the dataset if the share of outliers is acceptable.\\n')\n",
    "    return dataSkewKurt\n",
    "\n",
    "# Calculate skewness and kurtosis for numeric variables\n",
    "dataSkewKurt = getSkewKurt(data[numericPreds])\n",
    "\n",
    "# Button to view skewness and kurtosis values\n",
    "button = widgets.Button(description=\"View Skewness and Kurtosis values for all columns\",\n",
    "                        layout=Layout(width='50%'))\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        display(dataSkewKurt)\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Multivariate Outliers for Numeric Columns\n",
    "\n",
    "Outliers in the dataset can significantly affect the performance of any binary classification algorithm.\n",
    "\n",
    "**There is no rule of thumb for what share of outliers is acceptable. What is crucial is to be aware of the outliers present in the dataset.**\n",
    "\n",
    "**Method 1 - Standard Deviation (STD)**:\n",
    "For every column, calculate the mean and standard deviation (StD). Each data point is then compared with the mean as follows:\n",
    "`IF ABS(DATA_POINT - MEAN) > 2*StD THEN OUTLIER ELSE NOT_OUTLIER`\n",
    "\n",
    "**Method 2 - K-Nearest Neighbor (KNN)**:\n",
    "The KNN method uses the distance to its k-th nearest neighbor as the outlying score. PyOD supports three KNN detectors: largest, mean, and median, which use the distance of the k-th neighbor, the average of all the k neighbors, and the median distance to k neighbors as the outlying score, respectively.\n",
    "\n",
    "Execute the cell below to identify outliers using both methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "# Function to calculate outliers based on standard deviation\n",
    "def getNumericOutliers(data):\n",
    "    print('[INFO] Calculating outliers based on standard deviation')\n",
    "    dataOutlier = data.copy()\n",
    "    for col in dataOutlier:\n",
    "        dataOutlier[col + '_mean'] = dataOutlier[col].mean()\n",
    "        dataOutlier[col + '_std'] = dataOutlier[col].std()\n",
    "        dataOutlier[col + '_outlier'] = dataOutlier[[col, col + '_mean', col + '_std']].apply(\n",
    "            lambda x: 1 if abs(x[col] - x[col + '_mean']) > (x[col + '_std'] * 2) else 0, axis=1)\n",
    "\n",
    "    outlierCols = [col for col in dataOutlier.columns if '_outlier' in col]\n",
    "    dataOutlier['StdOutlier'] = dataOutlier[outlierCols].apply(lambda x: 1 if x.sum() > 1 else 0, axis=1)\n",
    "    dataOutlier.drop(columns=[col for col in dataOutlier.columns if '_mean' in col or '_std' in col or '_outlier' in col], inplace=True)\n",
    "    \n",
    "    outlier_fraction = dataOutlier['StdOutlier'].mean()\n",
    "    print(f'[INFO] Done. Count of outliers based on Std: {dataOutlier[\"StdOutlier\"].sum()}')\n",
    "    print(f'Share of outlier rows: {outlier_fraction:.2%}')\n",
    "    \n",
    "    return dataOutlier, outlier_fraction\n",
    "\n",
    "# Function to calculate outliers based on KNN\n",
    "def getKnnOutliers(X, fraction):\n",
    "    print('[INFO] Calculating outliers with KNN model')\n",
    "    clf = KNN(contamination=fraction)\n",
    "    clf.fit(X)\n",
    "    \n",
    "    # Predict raw anomaly score\n",
    "    scores_pred = clf.decision_function(X) * -1\n",
    "\n",
    "    # Prediction of a datapoint category outlier or inlier\n",
    "    y_pred = clf.predict(X).tolist()\n",
    "    \n",
    "    print('[INFO] Done. Count of outliers based on KNN:', sum(y_pred))\n",
    "    print('Share of outlier rows:', f'{sum(y_pred) / len(y_pred):.2%}')\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Calculate outliers based on standard deviation\n",
    "dataNumericOutlier, fraction = getNumericOutliers(data[numericPreds])\n",
    "\n",
    "# Prepare data for KNN outlier detection\n",
    "dataKnn = data[numericPreds].astype('float64')\n",
    "X = dataKnn.values\n",
    "\n",
    "# Calculate outliers based on KNN\n",
    "dataKnnOutlier = getKnnOutliers(X, fraction)\n",
    "dataKnn['KnnOutlier'] = dataKnnOutlier\n",
    "\n",
    "# Combine outlier results with the main dataset\n",
    "data['StdOutlier'] = dataNumericOutlier['StdOutlier']\n",
    "data['KnnOutlier'] = dataKnn['KnnOutlier']\n",
    "print('Columns [StdOutlier, KnnOutlier] were added to the dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Outliers\n",
    "\n",
    "Explore the columns with standard deviation outliers.\n",
    "\n",
    "Execute the cell below to view the rows identified as outliers by standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows with standard deviation outliers\n",
    "dataNumericOutlier[dataNumericOutlier['StdOutlier'] == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Description of KNN Outliers\n",
    "\n",
    "Execute the cell below to view the descriptive statistics of the rows identified as KNN outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display description of KNN outliers\n",
    "dataKnn[dataKnn['KnnOutlier'] == 1].describe().transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop/Replace/Keep Outliers\n",
    "\n",
    "If you have identified outliers in your dataset, you can choose to drop, replace, or keep them.\n",
    "\n",
    "<h3><font color=\"red\">↓↓↓ Execute the cell below and choose how to handle outliers ↓↓↓</font></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display widgets to choose how to handle outliers\n",
    "if len(data[data['StdOutlier'] == 1]) > 0 or len(data[data['KnnOutlier'] == 1]) > 0:\n",
    "    OUTLIER_ACTION = widgets.ToggleButtons(\n",
    "        options=['Keep', 'Drop'],\n",
    "        description='Action:',\n",
    "        disabled=False,\n",
    "        button_style='info',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "        value='Keep'\n",
    "    )\n",
    "\n",
    "    DETAIL_ACTION = widgets.ToggleButtons(\n",
    "        options=['All', 'Knn', 'Std'],\n",
    "        description='For:',\n",
    "        disabled=False,\n",
    "        button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "        value='All'\n",
    "    )\n",
    "    display(OUTLIER_ACTION)\n",
    "    display(DETAIL_ACTION)\n",
    "else:\n",
    "    print('[INFO] There are no outliers in your dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Outlier Handling Action\n",
    "\n",
    "Execute the cell below to apply the chosen action for handling outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to handle outliers based on selected action\n",
    "def solveOutliers(data, outlier_action_value, outlier_action_type):\n",
    "    messageOut = []\n",
    "    \n",
    "    if outlier_action_value == 'Keep':\n",
    "        messageOut.append('All outliers were kept in the dataset.')\n",
    "\n",
    "    elif outlier_action_value == 'Drop':\n",
    "        if outlier_action_type == 'All':\n",
    "            data = data[(data['StdOutlier'] == 0) & (data['KnnOutlier'] == 0)]\n",
    "            messageOut.append('All outliers were dropped.')\n",
    "            \n",
    "        elif outlier_action_type == 'Knn':\n",
    "            data = data[data['KnnOutlier'] == 0]\n",
    "            messageOut.append('KNN outliers were dropped.')\n",
    "            \n",
    "        elif outlier_action_type == 'Std':\n",
    "            data = data[data['StdOutlier'] == 0]\n",
    "            messageOut.append('STD outliers were dropped.')   \n",
    "    \n",
    "    print(messageOut)\n",
    "    return data\n",
    "\n",
    "# Apply the chosen action for handling outliers\n",
    "if len(data[data['StdOutlier'] == 1]) > 0 or len(data[data['KnnOutlier'] == 1]) > 0:\n",
    "    outlier_action_value = OUTLIER_ACTION.value\n",
    "    outlier_action_type = DETAIL_ACTION.value\n",
    "    data = solveOutliers(data, outlier_action_value, outlier_action_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Export transformed data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data.drop('StdOutlier', axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    data.drop('KnnOutlier', axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "data.to_csv('/data/data_transformed.csv', index=False)\n",
    "logging.info('Transformed dataset was loaded to the root folder of this workspace...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
