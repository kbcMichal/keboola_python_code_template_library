{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841fe642-4a27-4292-9fca-1628dfe752bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation for Machine Learning\n",
    "\n",
    "Welcome to this data preparation notebook. This notebook will guide you through the steps required to prepare your dataset for machine learning. The goal is to ensure that your data is clean, well-structured, and ready for modeling. We will cover the following steps:\n",
    "\n",
    "1. Importing necessary packages\n",
    "2. Loading the dataset\n",
    "3. Exploring and profiling the data\n",
    "4. Cleaning and preprocessing the data\n",
    "    - Removing redundant columns\n",
    "    - Solving duplicates\n",
    "    - Solving missing values\n",
    "    - Encoding categorical variables\n",
    "    - Feature scaling, Dimensionality reduction, Splitting dataset\n",
    "5. Exporting transformed dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fab1df-ca01-4927-948e-6a9e5a5c8d9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Importing Necessary Packages\n",
    "\n",
    "In this step, we will import all the necessary Python packages that will be used throughout the notebook. These packages include libraries for data manipulation, visualization, and profiling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eaf265-1256-4021-85f4-6287134f06ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib\n",
    "import sys\n",
    "import site\n",
    "import logging\n",
    "\n",
    "%matplotlib inline\n",
    "sys.path.append(site.getusersitepackages())\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from ipywidgets import widgets, Layout\n",
    "from keboola.component import CommonInterface\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c6e20-24a8-4fb5-9ed3-ffcd9c7d3450",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Selecting a Dataset\n",
    "\n",
    "In this step, we will list the tables that users have loaded into the workspace using the table input mapping. Users can then select the dataset they want to use for data profiling and exploration.\n",
    "\n",
    "The input datasets are loaded using the Keboola Common Interface, which allows seamless interaction with the data tables defined in the workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ae619-3de4-4721-ad73-ae804a2cc7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize CommonInterface\n",
    "ci = CommonInterface()\n",
    "\n",
    "# Load input tables\n",
    "input_tables = ci.get_input_tables_definitions()\n",
    "\n",
    "# List all CSV files in the input tables directory\n",
    "table_list = []\n",
    "for table in input_tables:\n",
    "    table_list.append(table.full_path)\n",
    "\n",
    "# Create a dropdown widget for selecting a table\n",
    "if table_list:\n",
    "    logging.info(\"Select the dataset you want to use from the dropdown.\")\n",
    "    tables = widgets.Dropdown(options=table_list, value=table_list[0],\n",
    "                              description='Table:', disabled=False)\n",
    "    display(tables)\n",
    "else:\n",
    "    logging.warning(\"No tables found. Please ensure you have loaded tables into the workspace using the table input mapping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1df2e-66b4-4bf6-b32d-b40f46777be3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Selected Dataset\n",
    "Once you have selected a dataset from the dropdown, this cell reads the CSV file into a pandas DataFrame and generates a profile report using the `ydata-profiling` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa304b-34ea-4c4b-989b-e236452863ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(tables.value)\n",
    "profile = ProfileReport(data)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650545bf-a182-46db-b48a-7da63ca8e68c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Alternatively Load Dataset from URL to follow the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d562882-fc76-446b-b4be-ab60ac4dc8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Titanic dataset\n",
    "titanic_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "\n",
    "# Load the Titanic dataset into a pandas DataFrame\n",
    "data = pd.read_csv(titanic_url)\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e2861-42fd-448f-9ceb-5d08c783b60f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Removing Redundant Columns\n",
    "\n",
    "We will now identify and remove columns that have only one unique value, as such columns are not useful for machine learning. You will be asked to confirm before any columns are removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b496fb8-4885-4ac0-a56c-344d619777a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify columns with only one unique value\n",
    "redundant_columns = [col for col in data.columns if data[col].nunique() <= 1]\n",
    "\n",
    "# Display redundant columns and ask for confirmation to drop\n",
    "if redundant_columns:\n",
    "    print(f\"The following columns have only one unique value and can be considered redundant: {redundant_columns}\")\n",
    "    drop_redundant = widgets.ToggleButtons(\n",
    "        options=['Yes', 'No'],\n",
    "        description='Drop Columns?',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    display(drop_redundant)\n",
    "else:\n",
    "    print(\"No redundant columns found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3527b-ef50-44ac-a962-179a4850591d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop redundant columns based on user confirmation\n",
    "if redundant_columns and drop_redundant.value == 'Yes':\n",
    "    data.drop(columns=redundant_columns, inplace=True)\n",
    "    print(f\"Dropped columns: {redundant_columns}\")\n",
    "else:\n",
    "    print(\"No columns were dropped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91edc03-1957-428b-bcba-b66c126e4bb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Solving Duplicate Rows\n",
    "\n",
    "We will identify duplicate rows in the dataset. You will be asked to confirm before any duplicates are removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346eced-1cb9-4261-9fa9-889b87d07a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify duplicate rows\n",
    "duplicate_rows = data.duplicated().sum()\n",
    "\n",
    "# Display duplicate rows count and ask for confirmation to drop\n",
    "if duplicate_rows > 0:\n",
    "    print(f\"There are {duplicate_rows} duplicate rows in the dataset.\")\n",
    "    drop_duplicates = widgets.ToggleButtons(\n",
    "        options=['Yes', 'No'],\n",
    "        description='Drop Duplicates?',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    display(drop_duplicates)\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75139bb2-8ed2-4413-8aec-9176370b5c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop duplicate rows based on user confirmation\n",
    "if duplicate_rows > 0 and drop_duplicates.value == 'Yes':\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    print(f\"Dropped {duplicate_rows} duplicate rows.\")\n",
    "else:\n",
    "    print(\"No duplicate rows were dropped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b3a78-c26a-487b-aeeb-45f0550250fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Solve Missing Values\n",
    "\n",
    "### Identify Missing Values\n",
    "\n",
    "In this section, we will identify the missing values in the dataset. This will help us understand the extent of missing data and decide on an appropriate action to handle it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7f592-8f02-4839-8a8d-8b97031789db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to identify missing values\n",
    "def getMissing(data):\n",
    "    missing_cnt = data.isna().sum().sum()\n",
    "    missing_pct = missing_cnt / (len(data.columns) * len(data))     \n",
    "    missing_out = data.isna().sum()\n",
    "    \n",
    "    print('=====================================')\n",
    "    print(f'Total missing cells: [{missing_cnt}]')\n",
    "    print(f'Percentage of missing cells: [{missing_pct:.2%}]')\n",
    "    print('=====================================')\n",
    "    print('Count of missing cells per column:')\n",
    "    print(missing_out)\n",
    "    print('=====================================')\n",
    "    print('-------------------------------------')\n",
    "\n",
    "# Identify missing values in the dataset\n",
    "getMissing(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2014e-0902-4df4-a309-fd63397a1058",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Decide How to Handle Missing Values\n",
    "\n",
    "Choose a missing action from the following options:\n",
    "- **\"drop\"**: Drop rows with missing values in the selected column(s).\n",
    "- **\"replace\"**: Replace missing numeric values with the MEAN and missing categorical values with a new category named \"Undefined\" for the selected columns.\n",
    "- **\"replaceNumeric\"**: Replace missing numeric values with the MEAN value for the selected columns.\n",
    "- **\"replaceCategorical\"**: Replace missing categorical values with a new category named \"Undefined\" for the selected columns.\n",
    "- **\"None\"**: Ignore missing values.\n",
    "\n",
    "<h3><font color=\"red\">↓↓↓ Execute the cell below and choose how to solve missing values ↓↓↓</font></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d9406-fe49-485b-b7d5-b224d7dc1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display widgets to choose how to handle missing values\n",
    "if data.isna().sum().sum() > 0:\n",
    "    MISSING_ACTION = widgets.ToggleButtons(\n",
    "        options=['None', 'drop', 'replace', 'replaceNumeric', 'replaceCategorical'],\n",
    "        description='Action:',\n",
    "        disabled=False,\n",
    "        button_style='info',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "        value='None'\n",
    "    )\n",
    "    COLUMNS_ACTION = widgets.SelectMultiple(\n",
    "        options=['ALL COLUMNS'] + list(data.columns),\n",
    "        description='Columns:',\n",
    "        ensure_option=True,\n",
    "        disabled=False,\n",
    "        rows=15\n",
    "    )\n",
    "\n",
    "    display(MISSING_ACTION)\n",
    "    display(COLUMNS_ACTION)\n",
    "else:\n",
    "    logging.info('[INFO] There are no missing values in your dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09877979-f688-4dcc-b028-6d98d2233d04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Apply Missing Values Action\n",
    "\n",
    "Execute the cell below to apply the chosen action for handling missing values.\n",
    "\n",
    "<i><b>NOTE:</b> You can select and execute the missing action multiple times.</i><br>\n",
    "<i>For example, you can first select 'drop' for a specific column and then 'replaceNumeric' for numeric columns you prefer not to drop.</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91443d74-db88-4a08-b74e-66e1eb834231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Function to handle missing values based on selected action\n",
    "def solveMissing(data, MISSING_ACTION):\n",
    "    messageOut = []\n",
    "    allColumns = list(data.columns)\n",
    "    datePreds = []\n",
    "    categoricalPreds = []\n",
    "    numericPreds = []\n",
    "    \n",
    "    for predictor in allColumns:\n",
    "        if data[predictor].dtype == 'object':\n",
    "            try:\n",
    "                pd.to_datetime(data[predictor])\n",
    "                datePreds.append(predictor)\n",
    "            except:\n",
    "                categoricalPreds.append(predictor) \n",
    "        elif 'datetime' in str(data[predictor].dtype):\n",
    "            datePreds.append(predictor)\n",
    "        else:\n",
    "            numericPreds.append(predictor)\n",
    "    \n",
    "    if 'None' in MISSING_ACTION[:4]:\n",
    "        messageOut.append('Not solving any columns.')\n",
    "        \n",
    "    if MISSING_ACTION == \"replaceAll\":\n",
    "        messageOut.append('Replacing missing values in all columns:')\n",
    "        for col in allColumns:\n",
    "            if data[col].isna().sum() > 0:\n",
    "                if col in numericPreds:\n",
    "                    data[col].fillna(data[col].mean(), inplace=True)\n",
    "                else:\n",
    "                    data[col].fillna('REPLACED-Undefined', inplace=True)\n",
    "                messageOut.append(col)\n",
    "            \n",
    "    elif \"replaceNumeric\" in MISSING_ACTION:\n",
    "        messageOut.append('Replacing missing values in NUMERIC columns:')\n",
    "        for col in numericPreds:\n",
    "            if data[col].isna().sum() > 0:\n",
    "                data[col].fillna(data[col].mean(), inplace=True)            \n",
    "                messageOut.append(col)\n",
    "            \n",
    "    elif \"replaceCategorical\" in MISSING_ACTION:\n",
    "        messageOut.append('Replacing missing values in CATEGORICAL columns:')\n",
    "        for col in categoricalPreds:\n",
    "            if data[col].isna().sum() > 0:\n",
    "                data[col].fillna('REPLACED-Undefined', inplace=True)\n",
    "                messageOut.append(col)\n",
    "    \n",
    "    elif \"replace\" in MISSING_ACTION:\n",
    "        messageOut.append('Replacing missing values in selected columns.')\n",
    "        colsToReplace = ast.literal_eval(MISSING_ACTION.replace(\"replace\", \"\"))\n",
    "        for col in colsToReplace:\n",
    "            if col in categoricalPreds:\n",
    "                if data[col].isna().sum() > 0:\n",
    "                    data[col].fillna('REPLACED-Undefined', inplace=True)\n",
    "                    messageOut.append(col)\n",
    "            else:\n",
    "                if data[col].isna().sum() > 0:\n",
    "                    data[col].fillna(data[col].mean(), inplace=True)            \n",
    "                    messageOut.append(col)\n",
    "                        \n",
    "    if MISSING_ACTION == 'dropAll':\n",
    "        messageOut.append('Dropping missing values in all columns.')\n",
    "        data.dropna(inplace=True)\n",
    "            \n",
    "    elif \"drop\" in MISSING_ACTION[:4]:\n",
    "        messageOut.append('Dropping missing values in selected columns.')\n",
    "        colsToDrop = ast.literal_eval(MISSING_ACTION.replace(\"drop\", \"\"))\n",
    "        data.dropna(subset=colsToDrop, inplace=True)\n",
    "        messageOut.append(colsToDrop)\n",
    "    \n",
    "    if len(messageOut) == 0:\n",
    "        messageOut.append('[INFO] There is nothing to do for selected action.')\n",
    "    print(messageOut)\n",
    "    return data\n",
    "\n",
    "# Apply the chosen action for handling missing values\n",
    "missing_action_value = MISSING_ACTION.value\n",
    "columns_action_value = list(COLUMNS_ACTION.value)\n",
    "if 'ALL COLUMNS' in columns_action_value:\n",
    "    missing_action_concat = missing_action_value + 'All'\n",
    "else:\n",
    "    missing_action_concat = missing_action_value + str(columns_action_value)\n",
    "\n",
    "data = solveMissing(data, missing_action_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795c79b-3264-41ff-a440-c25ab2740c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075dddd-25c6-42fb-8434-c7072e966de4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Identifying Data Types and Encoding Categorical Variables\n",
    "\n",
    "In this section, we will identify the data types of each column and suggest methods to encode categorical variables. Encoding categorical variables is a crucial step for machine learning as most algorithms require numerical input.\n",
    "\n",
    "### Identify Data Types\n",
    "\n",
    "We will first identify and display the data types of each column in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436afdb6-ab54-4f95-8e7d-7369e3b84fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify data types of each column\n",
    "data_types = data.dtypes\n",
    "print(\"Data types of each column:\")\n",
    "print(data_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2bb1c-7b79-494b-a699-37ffe05af81b",
   "metadata": {},
   "source": [
    "### Suggest Encoding Methods for Categorical Variables\n",
    "\n",
    "We have identified the categorical variables in the dataset. There are multiple methods to encode these variables:\n",
    "\n",
    "- **One-Hot Encoding**: Creates a new binary column for each unique category.\n",
    "- **Label Encoding**: Converts each category to a unique integer.\n",
    "\n",
    "You can select the encoding method for each categorical variable from the dropdown menus below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998162f3-d959-4677-a170-f2408633cda9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = [col for col in data.columns if data[col].dtype == 'object']\n",
    "\n",
    "# Create dropdown widgets for selecting encoding methods\n",
    "encoding_methods = ['One-Hot Encoding', 'Label Encoding']\n",
    "encoding_dropdowns = {col: widgets.Dropdown(options=encoding_methods, description=f'{col}:', disabled=False) for col in categorical_columns}\n",
    "\n",
    "# Display dropdown widgets\n",
    "for col, dropdown in encoding_dropdowns.items():\n",
    "    display(dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20928d-d6e5-4f0c-849b-68a734cdaa6b",
   "metadata": {},
   "source": [
    "### Apply Selected Encoding Methods\n",
    "\n",
    "Based on your selections above, we will encode the categorical variables using the chosen methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23355e24-f5b7-40fb-a538-4d633a4498bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply selected encoding methods\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col, dropdown in encoding_dropdowns.items():\n",
    "    method = dropdown.value\n",
    "    if method == 'One-Hot Encoding':\n",
    "        data = pd.get_dummies(data, columns=[col], drop_first=True)\n",
    "    elif method == 'Label Encoding':\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "\n",
    "print(\"Encoding applied. Here is the updated dataset:\")\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbff362-070d-4871-a730-34bd0ab3db21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Additional Steps Before Machine Learning\n",
    "\n",
    "To further prepare your dataset for machine learning, we will perform the following steps:\n",
    "\n",
    "1. **Select the Target Variable**: Identify the target variable, which is the variable we aim to predict.\n",
    "2. **Feature Scaling**: Normalize or standardize numerical features to ensure they have the same scale. This helps improve the performance of many machine learning algorithms that are sensitive to the scale of input data.\n",
    "3. **Feature Engineering**: Create new features from existing ones to help improve model performance. This step is optional and depends on the specific dataset and problem.\n",
    "4. **Dimensionality Reduction**: Reduce the number of features if you have a high-dimensional dataset, using techniques like PCA (Principal Component Analysis). This helps to reduce computational cost and can improve model performance.\n",
    "5. **Splitting the Data**: Split the dataset into training and testing sets to evaluate model performance. This helps in validating how well the model generalizes to unseen data.\n",
    "6. **Saving the Final Transformed Dataset**: Save the final transformed dataset to a CSV file for use in the machine learning notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f7c07-a8b8-4ff8-9fbc-aa43487b3ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Widget to select target column\n",
    "target_column_widget = widgets.Dropdown(\n",
    "    options=data.columns.tolist(),\n",
    "    description='Target Column:',\n",
    "    disabled=False\n",
    ")\n",
    "display(target_column_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b3741-1eec-4cd0-b3ae-ecedbf88fa6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select target column\n",
    "target_column = target_column_widget.value\n",
    "print(f\"Target column selected: {target_column}\")\n",
    "\n",
    "# Separate target column from features\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Ensure target remains binary if it is binary\n",
    "if y.nunique() == 2:\n",
    "    print(\"Target variable is binary and will remain unchanged.\")\n",
    "else:\n",
    "    print(\"Target variable is not binary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915e364-456f-46e6-bc0f-21e0b95dab1d",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Normalize or standardize numerical features to ensure they have the same scale. This helps improve the performance of many machine learning algorithms that are sensitive to the scale of input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f451b02-ec94-4d63-9a38-369bcf506d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
    "print(\"Feature scaling applied to numerical columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f251edf-eeb4-4058-b295-613968ee50a1",
   "metadata": {},
   "source": [
    "### Feature Engineering (Example: Creating a new feature based on existing ones)\n",
    " - Add any feature engineering steps here if applicable - this always has to be done specifically for your dataset so we don't provide any code\n",
    " - Typically you can calculate some aggregated values such as Count of orders in last 7 days, last 30 days, last 90 days etc. for every customer\n",
    "     - That gives you already 3 new features - it's obvious that there can be millions of valid features and feature engineering is very complex area\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27eee9-cd38-4efe-88fa-3976892fa9eb",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "Reduce the number of features if you have a high-dimensional dataset, using techniques like PCA (Principal Component Analysis). This helps to reduce computational cost and can improve model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bde95e-6de5-4f5a-b740-d4ef4d7d5139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dimensionality Reduction (Optional)\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_reduced = pca.fit_transform(X[numerical_columns])\n",
    "X_pca = pd.DataFrame(X_reduced, columns=[f'PC{i+1}' for i in range(X_reduced.shape[1])])\n",
    "\n",
    "# Combine PCA components with non-numerical columns (if any)\n",
    "non_numerical_data = X.drop(columns=numerical_columns)\n",
    "X_final = pd.concat([non_numerical_data.reset_index(drop=True), X_pca.reset_index(drop=True)], axis=1)\n",
    "print(\"Dimensionality reduction applied (if applicable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e61156-b1bf-4a42-8c12-ee2b93342806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c045f2-ec99-4011-88f5-8c28d582986d",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "Split the dataset into training and testing sets to evaluate the model performance. You can select the size of the test set from the options below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b79eb8-8698-4cce-82bc-e0b530c67938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Widget to select test size\n",
    "test_size_widget = widgets.FloatSlider(\n",
    "    value=0.2,\n",
    "    min=0.1,\n",
    "    max=0.5,\n",
    "    step=0.1,\n",
    "    description='Test Size:',\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal'\n",
    ")\n",
    "display(test_size_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2938cdf-e10c-46c1-bbf4-013b7f22075b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the Data\n",
    "test_size = test_size_widget.value\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=test_size, random_state=42)\n",
    "print(f\"Data split into training and testing sets with test size = {test_size}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885fd70-02f0-4eb4-b1a7-f3a13b33fe52",
   "metadata": {},
   "source": [
    "### Saving the Final Transformed Dataset\n",
    "\n",
    "We will save the final transformed dataset to a CSV file for use in the machine learning notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf29742-07a0-46b2-b783-55143220522c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the final transformed dataset\n",
    "final_dataset = pd.concat([X_final, y.reset_index(drop=True)], axis=1)\n",
    "final_dataset.to_csv('/data/final_transformed_dataset.csv', index=False)\n",
    "print(\"Final transformed dataset saved to /data/final_transformed_dataset.csv\")\n",
    "\n",
    "# Save the split train and test datasets\n",
    "train_dataset = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "test_dataset = pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_dataset.to_csv('/data/train_dataset.csv', index=False)\n",
    "print(\"Training dataset saved to /data/train_dataset.csv\")\n",
    "\n",
    "test_dataset.to_csv('/data/test_dataset.csv', index=False)\n",
    "print(\"Testing dataset saved to /data/test_dataset.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
