{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd65b39-a285-4d30-8e9f-b132fb46346f",
   "metadata": {},
   "source": [
    "## Working with Snowpark in Snowpark Python Workspace\n",
    "\n",
    "This notebook provides an introduction to using the Snowpark library within the Snowpark Python Workspace in Keboola. Snowpark allows you to build Python transformations that process data directly in Snowflake without moving the data out of it. This guide covers the basics of setting up Snowpark, executing SQL statements, and performing data transformations within Snowflake.\n",
    "\n",
    "### Key Features of Snowpark:\n",
    "\n",
    "- The Snowpark API provides intuitive programming constructs for building SQL statements.\n",
    "- Benefit from intelligent code completion and type checking when using native language constructs.\n",
    "- Snowpark supports pushdown for all operations, including Snowflake UDFs.\n",
    "- No need for a separate cluster outside of Snowflake for computations; all computations are performed within Snowflake.\n",
    "\n",
    "#### Additional Resources:\n",
    "- [Snowpark Overview](https://www.snowflake.com/snowpark/)\n",
    "- [Snowpark Python Developer Guide](https://docs.snowflake.com/en/developer-guide/snowpark/python/index.html)\n",
    "\n",
    "---\n",
    "\n",
    "### Snowpark in Keboola\n",
    "\n",
    "Snowpark is available as a new type of Python workspace and transformation in Keboola. It’s intended to provide a seamless experience for data processing and transformations within Snowflake. Below, we will guide you through setting up Snowpark, writing Snowpark code, and saving results back to Keboola Storage.\n",
    "\n",
    "Keboola Snowpark Python Workspace creates a dedicated Snowflake schema behind the scenes, with read permissions to the entire project Storage.\n",
    "\n",
    "*__TO USE SNOWPARK IN KEBOOLA, YOU NEED TO ENABLE THE SNOWPARK FEATURE IN YOUR PROJECT SETTINGS (OR CONTACT OUR SUPPORT TEAM) AND USE THIS NOTEBOOK IN SNOWPARK PYTHON WORKSPACE__*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729b1db-be30-464f-8691-f455cecc763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col\n",
    "from keboola.component import CommonInterface\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Setting up connection parameters from variables stored in the Workspace's environment\n",
    "ci = CommonInterface()\n",
    "connection_parameters = ci.configuration.workspace_credentials\n",
    "\n",
    "# Create a Snowpark session\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "logging.info(\"Snowpark session created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08bfdb1-6d56-45ea-8f6e-73f0cf68d519",
   "metadata": {},
   "source": [
    "---\n",
    "### Listing Tables in Snowflake\n",
    "\n",
    "In this section, we'll list all the tables available in the specified schema using Snowpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f28b7-97ef-4cda-a6af-7811c86905b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the schema\n",
    "tables = session.sql(\"SELECT * FROM INFORMATION_SCHEMA.TABLES\").collect()\n",
    "\n",
    "# Convert the list of tables to a DataFrame for better readability\n",
    "tables_df = pd.DataFrame(tables)\n",
    "\n",
    "# Display the tables\n",
    "display(tables_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79b585-f6ff-4236-abe4-13774051e581",
   "metadata": {},
   "source": [
    "---\n",
    "### Loading Data from a Snowflake Table\n",
    "\n",
    "Next, we’ll load data from a specific table into a DataFrame for processing. Replace 'your_table_name' with the actual table name you want to load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff0527-0547-438b-a6fc-c8c768c2f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a specific table\n",
    "table_name = 'your_table_name' # Example: '\"in.component-name\".\"table-name\"' -> notice that lowercase names of Schema and Table need to be quoted with \"\n",
    "df = session.table(table_name).to_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e952f9-0a27-4b03-99cc-bb168cf27918",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Transformation\n",
    "\n",
    "Perform data transformations using Snowpark. Here, we'll demonstrate some common transformations like filtering, selecting columns, and creating new columns.\n",
    "\n",
    "**Note:** Make sure to replace the placeholder column names with the actual column names from your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b04b5b-54b6-4205-b052-2e523589d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example transformation: Filter rows, select columns, and create a new column\n",
    "# Replace 'column_name', 'column1', 'column2', etc. with your actual column names\n",
    "\n",
    "# Filter rows where 'column_name' is greater than 100\n",
    "filtered_df = session.table(table_name).filter(col(\"column_name\") > 100)\n",
    "\n",
    "# Select specific columns 'column1' and 'column2'\n",
    "selected_df = filtered_df.select(col(\"column1\"), col(\"column2\"))\n",
    "\n",
    "# Create a new column 'new_column' as the sum of 'column1' and 'column2'\n",
    "transformed_df = selected_df.withColumn(\"new_column\", col(\"column1\") + col(\"column2\"))\n",
    "\n",
    "# Convert the transformed Snowpark DataFrame to a Pandas DataFrame\n",
    "result_df = transformed_df.to_pandas()\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "display(result_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de28d8f-8375-437d-b8d4-801a15fdb92f",
   "metadata": {},
   "source": [
    "---\n",
    "### Saving Results to Keboola Storage\n",
    "\n",
    "In Snowpark Python Workspaces, you don't use the `out/tables/` folder and CSV files to load results into storage. Instead, you need to write your outputs directly into the connected Snowflake workspace.\n",
    "\n",
    "We'll save the transformed data back to Snowflake as a new table. After writing the data to Snowflake, you can configure the Table Output Mapping in your transformation accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742be21-877b-430b-9d01-3b5f2260f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the output table in Snowflake\n",
    "output_table_name = 'your_output_table_name'  # Replace with your desired table name\n",
    "\n",
    "# Save the transformed DataFrame as a new table in Snowflake\n",
    "# Overwrite mode is used here; you can change it to 'append' if needed\n",
    "transformed_df.write.mode(\"overwrite\").save_as_table(output_table_name)\n",
    "\n",
    "logging.info(f\"Transformed data saved to Snowflake table: {output_table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001fe86e-c39f-45bd-8298-588adddb31c7",
   "metadata": {},
   "source": [
    "---\n",
    "# Full Transformation Example\n",
    "\n",
    "It is expected that your script will create new objects (tables) in the linked Snowflake workspace *as shown in the cells above*.\n",
    "\n",
    "You will then use a standard Table Output Mapping from your Snowpark Python Transformation. \n",
    "\n",
    "In our example we create a new table named __PACKAGE_NAME_DISTINCT__. \n",
    "\n",
    "## Optional: Dynamic output mapping\n",
    "Alternatively, instead of defining the Table Output Mapping in your Python Snowpark Transformation, you can produce a manifest file for the created objects and then you don't have to specify the output mapping.\n",
    "Please not that this might have an impact to the transparency of \"what is produced by the Transformation\" as it won't be immediatelly visible in the metadata definition of output mapping.\n",
    "\n",
    "```Python\n",
    "from keboola.component import CommonInterface\n",
    "\n",
    "ci = CommonInterface()\n",
    "\n",
    "# set destination as resulting output mapping, then the UI OM can be omitted\n",
    "result_table_id = \"out.c-my-new-bucket.PACKAGE_NAME_DISTINCT\"\n",
    "out_table_def = ci.create_out_table_definition(\"PACKAGE_NAME_DISTINCT\", primary_key=[], destination=result_table_id)\n",
    "ci.write_manifest(out_table_def)\n",
    "```\n",
    "\n",
    "___\n",
    "\n",
    "Following example is a full Transformation executable in your environment that'll produce a new table in bucket named __snowpark-demo__.\n",
    "\n",
    "```Python\n",
    "# ===== Imports =====\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import udf, col, lit, is_null, iff, initcap\n",
    "from keboola.component import CommonInterface\n",
    "\n",
    "# ===== Initiation =====\n",
    "\n",
    "# Setting up connection parameters from variables stored in the Workspace's environment\n",
    "ci = CommonInterface()\n",
    "connection_parameters = ci.configuration.workspace_credentials\n",
    "\n",
    "# Initiate the session\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(\"New Snowpark session created in a dedicated Snowflake workspace named\", session.get_current_schema())\n",
    "\n",
    "# ===== Main code - reading object from Snowflake and producing new table =====\n",
    "\n",
    "# Define the object\n",
    "df = session.table(\"INFORMATION_SCHEMA.PACKAGES\").filter(col(\"LANGUAGE\") == 'python').select(col(\"PACKAGE_NAME\")).distinct()\n",
    "\n",
    "# Write the object as a new table \n",
    "df.write.mode(\"overwrite\").save_as_table(\"PACKAGE_NAME_DISTINCT\")\n",
    "\n",
    "# ===== Define dynamic output mapping =====\n",
    "\n",
    "# set destination as resulting output mapping, then the UI OM can be omitted\n",
    "result_table_id = \"out.c-snowpark-demo.PACKAGE_NAME_DISTINCT\"\n",
    "out_table_def = ci.create_out_table_definition(\"PACKAGE_NAME_DISTINCT\", primary_key=[], destination=result_table_id)\n",
    "ci.write_manifest(out_table_def)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
